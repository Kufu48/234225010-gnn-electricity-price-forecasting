# -*- coding: utf-8 -*-
"""Ufuk KARAOĞLANDerin Öğrenme ve Uygulamaları  Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TEMdc45kLc19fO6aaUQkySJ7RR_5OBWC
"""

# Runtime > Change runtime type > GPU seç (varsa)

!pip -q install pandas numpy scikit-learn matplotlib tqdm
!pip -q install torch torchvision torchaudio
!pip -q install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.5.0+cu121.html
!pip -q install torch_geometric

import torch, torch_geometric
print("torch:", torch.__version__, "cuda:", torch.cuda.is_available(), "pyg:", torch_geometric.__version__)

from google.colab import files
uploaded = files.upload()  # "Derin Öğrenme ve Uygulamaları Veri Seti.xlsx" seç

import pandas as pd
import numpy as np

# Veriyi tekrar oku (emin olmak için)
df = pd.read_excel(fname)

def extract_hour(x):
    if pd.isna(x):
        return np.nan
    if hasattr(x, "hour"):          # datetime.time
        return x.hour
    try:                             # string veya int
        return int(str(x).split(":")[0])
    except:
        return np.nan

df["hour"] = df["Saat"].apply(extract_hour)

# Saat NaN olan satırları temizle (çok az olur, güvenli)
df = df.dropna(subset=["hour"]).reset_index(drop=True)

df["hour"] = df["hour"].astype(int)

# datetime oluştur
df["datetime"] = pd.to_datetime(df["Tarih"]) + pd.to_timedelta(df["hour"], unit="h")
df = df.sort_values("datetime").reset_index(drop=True)

print("Temizlenmiş veri boyutu:", df.shape)
df[["Tarih","Saat","hour","datetime"]].head(10)

cand = [c for c in df.columns if str(c).upper() in ["PTF","SMF"]]
print("Found targets:", cand)

print(df.columns.tolist())

import re

# Kolon adlarını normalize et (boşluk, parantez, noktalama vs.)
norm_map = {}
for c in df.columns:
    cc = str(c).strip()
    cc_norm = re.sub(r"\s+", " ", cc)  # çoklu boşlukları tek boşluk yap
    norm_map[c] = cc_norm

df = df.rename(columns=norm_map)

# İçerik araması: içinde PTF veya SMF geçen kolonları bul
cands = []
for c in df.columns:
    up = str(c).upper().replace("İ","I")  # TR-EN farkına küçük önlem
    if "PTF" in up or "SMF" in up:
        cands.append(c)

print("Target candidates:", cands)

import pandas as pd
import numpy as np

TARGET_COL = "ptf_fiyat"   # önce PTF

GEN_COLS = [
    'Hidroelektrik (HES)', 'Rüzgar (RES)', 'Güneş (GES)', 'Jeotermal (JES)',
    'Biyokütle/Biyogaz', 'Doğalgaz', 'Kömür', 'Sıvı Yakıt', 'Sıvılaştırılmış Gaz'
]

# numeric + temizle
df[GEN_COLS] = df[GEN_COLS].apply(pd.to_numeric, errors="coerce").fillna(0.0).clip(lower=0.0)
df[TARGET_COL] = pd.to_numeric(df[TARGET_COL], errors="coerce")

df = df.dropna(subset=[TARGET_COL]).reset_index(drop=True)

print("Rows:", len(df))
print("Target:", TARGET_COL, "min/max:", df[TARGET_COL].min(), df[TARGET_COL].max())

df["dow"] = df["datetime"].dt.dayofweek
df["hour_sin"] = np.sin(2*np.pi*df["hour"]/24)
df["hour_cos"] = np.cos(2*np.pi*df["hour"]/24)
df["dow_sin"]  = np.sin(2*np.pi*df["dow"]/7)
df["dow_cos"]  = np.cos(2*np.pi*df["dow"]/7)
TIME_COLS = ["hour_sin","hour_cos","dow_sin","dow_cos"]

split = int(len(df)*0.8)
train_df = df.iloc[:split].reset_index(drop=True)
test_df  = df.iloc[split:].reset_index(drop=True)
print("train:", train_df.shape, "test:", test_df.shape)

from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

Xtr = train_df[GEN_COLS + TIME_COLS].values
ytr = train_df[TARGET_COL].values

Xte = test_df[GEN_COLS + TIME_COLS].values
yte = test_df[TARGET_COL].values

base = Ridge(alpha=1.0)
base.fit(Xtr, ytr)
pred_base = base.predict(Xte)

mse  = mean_squared_error(yte, pred_base)
rmse = np.sqrt(mse)
mae  = mean_absolute_error(yte, pred_base)
r2   = r2_score(yte, pred_base)

print("Baseline Ridge -> RMSE:", rmse, "| MAE:", mae, "| R2:", r2)

plt.figure(figsize=(11,4))
plt.plot(test_df["datetime"], yte, label="true")
plt.plot(test_df["datetime"], pred_base, label="baseline")
plt.title(f"Baseline prediction ({TARGET_COL})")
plt.legend(); plt.tight_layout(); plt.show()

import torch
from torch_geometric.data import Data

L = 24  # geçmiş pencere (istersen 48 yap)

num_nodes = len(GEN_COLS)
edges = [[i,j] for i in range(num_nodes) for j in range(num_nodes)]
edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()

def make_graph_samples(frame):
    data_list = []
    for t in range(L, len(frame)):
        hist = frame.iloc[t-L:t]

        x_list = []
        for c in GEN_COLS:
            seq = hist[c].values.astype(np.float32)                  # (L,)
            tf  = hist[TIME_COLS].iloc[-1].values.astype(np.float32) # (4,)
            x_list.append(np.concatenate([seq, tf]))                 # (L+4,)

        x = torch.tensor(np.stack(x_list), dtype=torch.float32)      # (nodes, L+4)
        y = torch.tensor([frame.iloc[t][TARGET_COL]], dtype=torch.float32)  # (1,)
        data_list.append(Data(x=x, edge_index=edge_index, y=y))
    return data_list

train_ds = make_graph_samples(train_df)
test_ds  = make_graph_samples(test_df)

print("train graphs:", len(train_ds), "test graphs:", len(test_ds))

import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from torch_geometric.loader import DataLoader
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("device:", device)

train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
test_loader  = DataLoader(test_ds, batch_size=64, shuffle=False)

class GATPrice(torch.nn.Module):
    def __init__(self, in_dim, hid=64, heads=4):
        super().__init__()
        self.gat1 = GATConv(in_dim, hid, heads=heads, dropout=0.2)
        self.gat2 = GATConv(hid*heads, hid, heads=1, dropout=0.2)
        self.lin  = torch.nn.Linear(hid, 1)

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        x = F.elu(self.gat1(x, edge_index))
        x = F.elu(self.gat2(x, edge_index))
        g = global_mean_pool(x, batch)
        return self.lin(g).squeeze(-1)

model = GATPrice(in_dim=L+4).to(device)
opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)

def run_eval(loader):
    model.eval()
    ys, ps = [], []
    with torch.no_grad():
        for data in loader:
            data = data.to(device)
            p = model(data)
            ys.append(data.y.squeeze(-1).detach().cpu().numpy())
            ps.append(p.detach().cpu().numpy())
    return np.concatenate(ys), np.concatenate(ps)

EPOCHS = 20
for ep in range(1, EPOCHS+1):
    model.train()
    total = 0.0
    for data in train_loader:
        data = data.to(device)
        opt.zero_grad()
        p = model(data)
        loss = F.mse_loss(p, data.y.squeeze(-1))
        loss.backward()
        opt.step()
        total += loss.item()

    if ep % 5 == 0 or ep == 1:
        y, p = run_eval(test_loader)
        mse  = mean_squared_error(y, p)
        rmse = np.sqrt(mse)
        mae  = mean_absolute_error(y, p)
        r2   = r2_score(y, p)
        print(f"Epoch {ep:02d} | train_loss {total/len(train_loader):.4f} | RMSE {rmse:.2f} MAE {mae:.2f} R2 {r2:.3f}")

# Final plot
y, p = run_eval(test_loader)

plt.figure(figsize=(11,4))
plt.plot(test_df["datetime"].iloc[L:], y, label="true")
plt.plot(test_df["datetime"].iloc[L:], p, label="GAT")
plt.title(f"GAT prediction ({TARGET_COL})")
plt.legend(); plt.tight_layout(); plt.show()

"""Normalizasyon"""

from sklearn.preprocessing import StandardScaler
import numpy as np

# 1) önceki fiyat (lag)
df["prev_price"] = df[TARGET_COL].shift(1)
df = df.dropna(subset=["prev_price"]).reset_index(drop=True)

# time split'i yeniden kur (shift yüzünden 1 satır gitti)
split = int(len(df)*0.8)
train_df = df.iloc[:split].reset_index(drop=True)
test_df  = df.iloc[split:].reset_index(drop=True)

# 2) scaler: sadece train ile fit (doğru yöntem)
x_scaler = StandardScaler()
y_scaler = StandardScaler()

train_X_raw = train_df[GEN_COLS + TIME_COLS + ["prev_price"]].values
test_X_raw  = test_df[GEN_COLS + TIME_COLS + ["prev_price"]].values

train_y_raw = train_df[[TARGET_COL]].values
test_y_raw  = test_df[[TARGET_COL]].values

x_scaler.fit(train_X_raw)
y_scaler.fit(train_y_raw)

train_X = x_scaler.transform(train_X_raw)
test_X  = x_scaler.transform(test_X_raw)

train_y = y_scaler.transform(train_y_raw).reshape(-1)
test_y  = y_scaler.transform(test_y_raw).reshape(-1)

print("OK: normalized + prev_price ready")

import torch
from torch_geometric.data import Data

L = 24  # pencere

num_nodes = len(GEN_COLS)
edges = [[i,j] for i in range(num_nodes) for j in range(num_nodes)]
edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()

def make_graph_samples_from_scaled(frame_X, frame_y, frame_dt):
    data_list = []
    for t in range(L, len(frame_y)):
        # geçmiş pencere [t-L, ..., t-1]
        hist_X = frame_X[t-L:t]  # (L, feat_total)

        # hist_X içinde kolon sırası: GEN_COLS + TIME_COLS + prev_price
        # Biz node başına: [L üretim seq] + [time(4)] + [prev_price(1)]
        # time ve prev_price: t-1 satırından alıyoruz (son saat)
        last = hist_X[-1]
        time_feat = last[len(GEN_COLS):len(GEN_COLS)+len(TIME_COLS)]           # (4,)
        prev_p    = last[len(GEN_COLS)+len(TIME_COLS):len(GEN_COLS)+len(TIME_COLS)+1]  # (1,)

        x_list = []
        for i, c in enumerate(GEN_COLS):
            seq = hist_X[:, i].astype(np.float32)  # (L,) sadece o kaynağın geçmişi
            feat = np.concatenate([seq, time_feat.astype(np.float32), prev_p.astype(np.float32)])  # (L+4+1,)
            x_list.append(feat)

        x = torch.tensor(np.stack(x_list), dtype=torch.float32)  # (nodes, L+5)
        y = torch.tensor([frame_y[t]], dtype=torch.float32)      # scaled target
        data_list.append(Data(x=x, edge_index=edge_index, y=y))
    return data_list

train_dt = train_df["datetime"].values
test_dt  = test_df["datetime"].values

train_ds = make_graph_samples_from_scaled(train_X, train_y, train_dt)
test_ds  = make_graph_samples_from_scaled(test_X, test_y, test_dt)

print("train graphs:", len(train_ds), "test graphs:", len(test_ds))

import torch
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from torch_geometric.loader import DataLoader
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
test_loader  = DataLoader(test_ds, batch_size=64, shuffle=False)

class GATPrice(torch.nn.Module):
    def __init__(self, in_dim, hid=64, heads=4):
        super().__init__()
        self.gat1 = GATConv(in_dim, hid, heads=heads, dropout=0.2)
        self.gat2 = GATConv(hid*heads, hid, heads=1, dropout=0.2)
        self.lin  = torch.nn.Linear(hid, 1)

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        x = F.elu(self.gat1(x, edge_index))
        x = F.elu(self.gat2(x, edge_index))
        g = global_mean_pool(x, batch)
        return self.lin(g).squeeze(-1)

model = GATPrice(in_dim=L+5).to(device)
opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)

def run_eval(loader):
    model.eval()
    ys, ps = [], []
    with torch.no_grad():
        for data in loader:
            data = data.to(device)
            p = model(data)
            ys.append(data.y.squeeze(-1).detach().cpu().numpy())
            ps.append(p.detach().cpu().numpy())
    return np.concatenate(ys), np.concatenate(ps)

EPOCHS = 25
for ep in range(1, EPOCHS+1):
    model.train()
    total = 0.0
    for data in train_loader:
        data = data.to(device)
        opt.zero_grad()
        p = model(data)
        loss = F.mse_loss(p, data.y.squeeze(-1))
        loss.backward()
        opt.step()
        total += loss.item()

    if ep % 5 == 0 or ep == 1:
        y_s, p_s = run_eval(test_loader)
        # scaled -> gerçek fiyat
        y_real = y_scaler.inverse_transform(y_s.reshape(-1,1)).reshape(-1)
        p_real = y_scaler.inverse_transform(p_s.reshape(-1,1)).reshape(-1)

        mse = mean_squared_error(y_real, p_real)
        rmse = np.sqrt(mse)
        mae  = mean_absolute_error(y_real, p_real)
        r2   = r2_score(y_real, p_real)
        print(f"Epoch {ep:02d} | train_loss {total/len(train_loader):.4f} | RMSE {rmse:.2f} MAE {mae:.2f} R2 {r2:.3f}")

# final plot
y_s, p_s = run_eval(test_loader)
y_real = y_scaler.inverse_transform(y_s.reshape(-1,1)).reshape(-1)
p_real = y_scaler.inverse_transform(p_s.reshape(-1,1)).reshape(-1)

plt.figure(figsize=(11,4))
plt.plot(test_df["datetime"].iloc[L:], y_real, label="true")
plt.plot(test_df["datetime"].iloc[L:], p_real, label="GAT (norm+prev)")
plt.title(f"GAT prediction ({TARGET_COL}) - normalized + prev_price")
plt.legend(); plt.tight_layout(); plt.show()

import numpy as np
import pandas as pd
import torch
from torch_geometric.loader import DataLoader

num_nodes = len(GEN_COLS)

# test graph'ları tek tek gez
test_loader_1 = DataLoader(test_ds, batch_size=1, shuffle=False)

out_sum = np.zeros(num_nodes, dtype=np.float64)
out_cnt = np.zeros(num_nodes, dtype=np.float64)
in_sum  = np.zeros(num_nodes, dtype=np.float64)
in_cnt  = np.zeros(num_nodes, dtype=np.float64)

model.eval()
with torch.no_grad():
    for data in test_loader_1:
        data = data.to(device)

        # 1. katman attention
        _, (eidx, alpha) = model.gat1(data.x, data.edge_index, return_attention_weights=True)

        if alpha.dim() == 2:
            a = alpha.mean(dim=1).detach().cpu().numpy()   # (E,)
        else:
            a = alpha.view(-1).detach().cpu().numpy()

        src = eidx[0].detach().cpu().numpy()
        dst = eidx[1].detach().cpu().numpy()

        # batch_size=1 olduğundan src/dst kesin 0..num_nodes-1 aralığında
        np.add.at(out_sum, src, a); np.add.at(out_cnt, src, 1)
        np.add.at(in_sum,  dst, a); np.add.at(in_cnt,  dst, 1)

out_mean = out_sum / np.maximum(out_cnt, 1)
in_mean  = in_sum  / np.maximum(in_cnt,  1)

att_df = pd.DataFrame({
    "node": GEN_COLS,
    "outgoing_att_mean": out_mean,
    "incoming_att_mean": in_mean
}).sort_values("outgoing_att_mean", ascending=False)

display(att_df)

# RES & GES net skorlar
res_out = float(att_df.loc[att_df["node"]=="Rüzgar (RES)", "outgoing_att_mean"].values[0])
res_in  = float(att_df.loc[att_df["node"]=="Rüzgar (RES)", "incoming_att_mean"].values[0])
ges_out = float(att_df.loc[att_df["node"]=="Güneş (GES)",  "outgoing_att_mean"].values[0])
ges_in  = float(att_df.loc[att_df["node"]=="Güneş (GES)",  "incoming_att_mean"].values[0])

print("\nRES attention  -> outgoing:", res_out, "| incoming:", res_in)
print("GES attention  -> outgoing:", ges_out, "| incoming:", ges_in)

import numpy as np
import matplotlib.pyplot as plt
from torch_geometric.loader import DataLoader

num_nodes = len(GEN_COLS)
node2idx = {n:i for i,n in enumerate(GEN_COLS)}
idx_RES = node2idx["Rüzgar (RES)"]
idx_GES = node2idx["Güneş (GES)"]

# Batch offset problemi olmasın diye batch_size=1
test_loader_1 = DataLoader(test_ds, batch_size=1, shuffle=False)

# RES -> dst attention biriktir
res_to_sum = np.zeros(num_nodes, dtype=np.float64)
res_to_cnt = np.zeros(num_nodes, dtype=np.float64)

# GES -> dst attention biriktir
ges_to_sum = np.zeros(num_nodes, dtype=np.float64)
ges_to_cnt = np.zeros(num_nodes, dtype=np.float64)

model.eval()
with torch.no_grad():
    for data in test_loader_1:
        data = data.to(device)

        # 1. katman attention
        _, (eidx, alpha) = model.gat1(data.x, data.edge_index, return_attention_weights=True)

        # alpha: (E, heads) -> head ortalaması
        if alpha.dim() == 2:
            a = alpha.mean(dim=1).detach().cpu().numpy()
        else:
            a = alpha.view(-1).detach().cpu().numpy()

        src = eidx[0].detach().cpu().numpy()
        dst = eidx[1].detach().cpu().numpy()

        # RES kaynaklı edge'ler
        mask_res = (src == idx_RES)
        np.add.at(res_to_sum, dst[mask_res], a[mask_res])
        np.add.at(res_to_cnt, dst[mask_res], 1)

        # GES kaynaklı edge'ler
        mask_ges = (src == idx_GES)
        np.add.at(ges_to_sum, dst[mask_ges], a[mask_ges])
        np.add.at(ges_to_cnt, dst[mask_ges], 1)

# Ortalama attention (RES->dst, GES->dst)
res_to_mean = res_to_sum / np.maximum(res_to_cnt, 1)
ges_to_mean = ges_to_sum / np.maximum(ges_to_cnt, 1)

# Self-loop'u istersen çıkar (yorumlaması daha temiz olur)
# res_to_mean[idx_RES] = 0
# ges_to_mean[idx_GES] = 0

# Sıralama: en çok attention alan hedefler
order_res = np.argsort(res_to_mean)[::-1]
order_ges = np.argsort(ges_to_mean)[::-1]

# ---- Plot: RES dağılımı
plt.figure(figsize=(12,4))
plt.bar([GEN_COLS[i] for i in order_res], res_to_mean[order_res])
plt.xticks(rotation=75, ha="right")
plt.title("RES (Rüzgar) → Diğer kaynaklara attention dağılımı (ortalama, test seti)")
plt.ylabel("Attention (mean)")
plt.tight_layout()
plt.show()

# ---- Plot: GES dağılımı
plt.figure(figsize=(12,4))
plt.bar([GEN_COLS[i] for i in order_ges], ges_to_mean[order_ges])
plt.xticks(rotation=75, ha="right")
plt.title("GES (Güneş) → Diğer kaynaklara attention dağılımı (ortalama, test seti)")
plt.ylabel("Attention (mean)")
plt.tight_layout()
plt.show()

# İstersen top-5 hedefleri yazdır
top5_res = [(GEN_COLS[i], float(res_to_mean[i])) for i in order_res[:5]]
top5_ges = [(GEN_COLS[i], float(ges_to_mean[i])) for i in order_ges[:5]]

print("Top-5 RES ->", top5_res)
print("Top-5 GES ->", top5_ges)

"""Eğitim Döngüsü"""

import os

os.makedirs("results/figures", exist_ok=True)
print("results/figures hazır")

"""Eğitim döngüsünü LOG"""

train_loss_hist = []
rmse_hist = []
mae_hist  = []
r2_hist   = []

EPOCHS = 25

for ep in range(1, EPOCHS+1):
    model.train()
    epoch_loss = 0.0

    for data in train_loader:
        data = data.to(device)
        opt.zero_grad()
        p = model(data)
        loss = F.mse_loss(p, data.y.squeeze(-1))
        loss.backward()
        opt.step()
        epoch_loss += loss.item()

    epoch_loss /= len(train_loader)
    train_loss_hist.append(epoch_loss)

    # ---- test evaluation ----
    y_s, p_s = run_eval(test_loader)

    # scaled -> gerçek
    y_real = y_scaler.inverse_transform(y_s.reshape(-1,1)).reshape(-1)
    p_real = y_scaler.inverse_transform(p_s.reshape(-1,1)).reshape(-1)

    mse = mean_squared_error(y_real, p_real)
    rmse = np.sqrt(mse)
    mae  = mean_absolute_error(y_real, p_real)
    r2   = r2_score(y_real, p_real)

    rmse_hist.append(rmse)
    mae_hist.append(mae)
    r2_hist.append(r2)

    print(f"Epoch {ep:02d} | loss {epoch_loss:.4f} | RMSE {rmse:.2f} | MAE {mae:.2f} | R2 {r2:.3f}")

"""Loss Curve"""

import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
plt.plot(train_loss_hist, label="Train Loss (MSE)")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss Curve")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("results/figures/loss_curve.png", dpi=150)
plt.show()

"""RMSE / MAE Eğrileri"""

plt.figure(figsize=(6,4))
plt.plot(rmse_hist, label="RMSE")
plt.plot(mae_hist, label="MAE")
plt.xlabel("Epoch")
plt.ylabel("Error")
plt.title("Test Error Metrics")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("results/figures/error_metrics_curve.png", dpi=150)
plt.show()

"""R² Eğrisi"""

plt.figure(figsize=(6,4))
plt.plot(r2_hist, label="R² Score")
plt.xlabel("Epoch")
plt.ylabel("R²")
plt.title("Test R² Curve")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("results/figures/r2_curve.png", dpi=150)
plt.show()

"""JSON olarak kaydı"""

import json

metrics = {
    "final_rmse": rmse_hist[-1],
    "final_mae": mae_hist[-1],
    "final_r2": r2_hist[-1],
    "rmse_per_epoch": rmse_hist,
    "mae_per_epoch": mae_hist,
    "r2_per_epoch": r2_hist
}

with open("results/metrics.json", "w") as f:
    json.dump(metrics, f, indent=2)

print("metrics.json kaydedildi")

"""GİTHUB Bağlantısı"""